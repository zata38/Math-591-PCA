\documentclass{beamer}
\usepackage{etex}

\usetheme{CambridgeUS}
\setbeamertemplate{mini frames}{}
\usepackage[T1]{fontenc}
\usepackage{amssymb,amsfonts,amsmath}
\usepackage{epstopdf}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{epstopdf}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes}
\usepackage{xifthen}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{latexsym}
\usepackage[dvips]{pstricks} % PSTricks
\usepackage{pst-node}
\usepackage{psfrag}
\usepackage{wrapfig}
\usepackage{alltt}
\usepackage{url}
\usepackage{pict2e}
\usepackage{multimedia}
\usepackage{fancyvrb}
\usepackage{graphics}
\usepackage{verbatim}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage[angle=0,scale=5,opacity=1,color=black]{background}


\begin{document}
% \selectlanguage{english}
\selectlanguage{english}
\title[PCA in Symbol Recognition]{Applications of Principle Component Analysis in the Recognition of Handwritten Symbols}
\author[Group \# 1]{Z. Atkins, A. Barrett, B. Streit}
\institute[MATH 591]{
{University of Kansas}\\
Department of Mathematics
}
\date[Group \#1]{\small \textbf{MATH 591} - Spring 2018\\
Supervisors: Dr. A. Mi\k{e}dlar, A. Kolasinski\\
March 27, 2018
}
\titlegraphic{\hspace{0in}\includegraphics[width=2.0cm,height=1.5cm]{uku_logo.png}\;
\hspace{3.2in}\includegraphics[width=1.4cm,height=1.4cm]{ku_logo.jpg}\;
}


\frame{
\titlepage

}


\frame{
\frametitle{Introduction}
Key Problem: How to algorithmically identify handwritten symbols
\begin{enumerate}
    \item Stage 1: Handwritten digit recognition
    \begin{itemize}
        \item Current Dataset: MNIST handwritten image database \cite{data:MNIST}
        \item Other datasets: CEDAR CDROM 1 \cite{Hinton:1994:RHD:2998687.2998813}
        \item Well-solved problem -- good test of algorithms
        
    \end{itemize}
    \item Stage 2: Handwritten music notation
\end{enumerate}

}

\frame{
\frametitle{Challenges of Classification}
Classification of images is a computationally expensive process:
\begin{itemize}
    \item Each image is a $m\times n$ matrix of pixels
    \item For even small $m,n$, have very high dimensional data 
    \item E.g. with $28\times 28$ pixel images, we have $784$ dimensions
\end{itemize}
This scales very poorly: for $640\times 480$ pixels, we have $307\,200$ dimensions.

This problem is presented in ``Eigenfaces for Recognition'' in the context of image-based facial recognition \cite{doi:10.1162/jocn.1991.3.1.71}.
}

\frame{
\frametitle{Dimensionality Reduction with Principal Component Analysis (PCA)}
How do we make classification computationally feasible?\pause

Dimensionality reduction:\pause For $p$ images with dimension $m\times n$:
\begin{enumerate}
    \item Transform matrix of image matrices ($m\times n\times p$) into matrix $A$ of image vectors ($mn\times p$)\pause
    \item Center data such that $\mathop{mean}(A(:,i)) = 0$ for $i = 1,\dots,m\cdot n$\pause
    \item Compute the covariance matrix $C = \mathop{cov}(A) = \dfrac{1}{n-1}(A\cdot A^*)$\pause
    \item The sorted eigenvalues and eigenvectors of $C$ are the \textit{principal values} and \textit{principal component vectors} of A, resp.\pause
    \item Take first $k$ principal component vectors as $k\times k$ matrix $R$, this is the reduced \textit{feature space} of our data
\end{enumerate}

This process is explained thoroughly in \cite{DBLP:journals/corr/Shlens14} and \cite{doi:10.1162/jocn.1991.3.1.71}.
}

\frame{
\frametitle{Assumptions of PCA}
Linearity of input data
\begin{itemize}
    \item PCA is just a change of basis -- only makes sense if data is linear
    \item Can be avoided if non-linearity of data is known by transforming data before preforming PCA \cite{DBLP:journals/corr/Shlens14}
\end{itemize}\pause

High variance implies structure
\begin{itemize}
    \item Because we use the covariance matrix to choose principal components, we rely on variance as a means of distinguishing between structure and noise -- may fail \cite{DBLP:journals/corr/Shlens14}
\end{itemize}

}

\frame{
\frametitle{Dataset}
MNIST Image Database
\begin{itemize}
    \item Subset of NIST database with all images centered and scaled to same size \cite{data:MNIST}
    \item Provides consistent data for testing of model 
    \item Train set: $60\,000$ images from NIST train
    \item Test set: $10\,000$ images, $5000$ from NIST train, $5000$ from NIST test.
\end{itemize}

}

\frame{
\frametitle{Numerical Results}
MNIST Image Database:

\medskip
\begin{tabular}{||r|r|r||}\hline\hline
    Classifier & $\mathop{dim}(R)$ & Raw Errors / $\%$ Error \\\hline\hline
    2-norm distance from class mean: & $10$ & $2465\, $/$\,24.65\,\%$\\\hline
    & $20$ & $2063\, $/$\,20.63\%$\\\hline
    & $40$ & $1789\, $/$\, 17.89\%$\\\hline
    & $60$ & $1807\, $/$\, 18.07\%$\\\hline\hline
    Nearest-Neighbor: & $10$ &$898\, $/$\,8.98\,\%$\\ \hline
    & $20$ &$384\, $/$\,3.84\,\%$\\ \hline
    & $40$ &$307\, $/$\,3.07\,\%$\\ \hline
    & $60$ &$382\, $/$\,3.82\,\%$\\ \hline\hline
\end{tabular}

\medskip
Optimal feature space for classification is between $20$ and $60$ dimensions.
}


\section*{Bibliography}

\frame{
\frametitle{Bibliography}
\begin{footnotesize}
\bibliographystyle{amsalpha} 
\bibliography{literature} 
\end{footnotesize}
}

\end{document}

